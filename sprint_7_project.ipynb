{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "This project analyzes user behavior data to predict whether a user is classified as \"is_ultra\". The goals are to explore and clean the data, engineer consistent feature scales, establish baseline performance, and compare several classification methods to identify a practical model for deployment.\n",
    "\n",
    "Methods used: Data import and EDA; feature scaling and preprocessing; an 80/20 train/test split with a further validation split for tuning; baseline checks (mean and DummyClassifier); and model comparisons (Logistic Regression, Random Forest with n_estimators search, Decision Tree with max_depth search). Model selection was based on validation and final test accuracy, with Decision Tree recommended for initial deployment due to faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/datasets/users_behavior.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imported dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls  minutes  messages   mb_used  is_ultra\n",
      "0   40.0   311.90      83.0  19915.42         0\n",
      "1   85.0   516.75      56.0  22696.96         0\n",
      "2   77.0   467.66      86.0  21060.45         0\n",
      "3  106.0   745.53      81.0   8437.39         1\n",
      "4   66.0   418.74       1.0  14502.75         0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n",
      "None\n",
      "             calls      minutes     messages       mb_used     is_ultra\n",
      "count  3214.000000  3214.000000  3214.000000   3214.000000  3214.000000\n",
      "mean     63.038892   438.208787    38.281269  17207.673836     0.306472\n",
      "std      33.236368   234.569872    36.148326   7570.968246     0.461100\n",
      "min       0.000000     0.000000     0.000000      0.000000     0.000000\n",
      "25%      40.000000   274.575000     9.000000  12491.902500     0.000000\n",
      "50%      62.000000   430.600000    30.000000  16943.235000     0.000000\n",
      "75%      82.000000   571.927500    57.000000  21424.700000     1.000000\n",
      "max     244.000000  1632.060000   224.000000  49745.730000     1.000000\n",
      "0    2229\n",
      "1     985\n",
      "Name: is_ultra, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df['is_ultra'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data structure revealed some columns with large value ranges. I will rescale certain features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls      hours  messages   gb_used  is_ultra\n",
      "0   40.0   5.198333      83.0  19.91542         0\n",
      "1   85.0   8.612500      56.0  22.69696         0\n",
      "2   77.0   7.794333      86.0  21.06045         0\n",
      "3  106.0  12.425500      81.0   8.43739         1\n",
      "4   66.0   6.979000       1.0  14.50275         0\n"
     ]
    }
   ],
   "source": [
    "df['mb_used'] = df['mb_used'] / 1024\n",
    "df.rename(columns = {'mb_used' : 'gb_used'}, inplace = True)\n",
    "df['minutes'] = df['minutes'] / 60\n",
    "df.rename(columns = {'minutes' : 'hours'}, inplace = True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converted MB to GB and minutes to hours to produce more comparable feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tv, df_test = train_test_split(df, test_size = 0.20, random_state = 17)\n",
    "df_train, df_valid = train_test_split(df_tv, test_size = 0.25, random_state = 17)\n",
    "feature_train = df_train.drop(['is_ultra'], axis = 1)\n",
    "target_train = df_train['is_ultra']\n",
    "feature_valid = df_valid.drop(['is_ultra'], axis = 1)\n",
    "target_valid = df_valid['is_ultra']\n",
    "feature_test = df_test.drop(['is_ultra'], axis = 1)\n",
    "target_test = df_test['is_ultra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up feature and target variables and split the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.6765163297045101\n"
     ]
    }
   ],
   "source": [
    "baseline_prediction = target_train.mean()\n",
    "baseline_predictions = pd.Series(baseline_prediction, index = target_valid.index)\n",
    "baseline_accuracy = accuracy_score(target_valid, baseline_predictions.round())\n",
    "print(f\"Baseline accuracy: {baseline_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline prediction using the mean to provide a comparison point for models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random guessing accuracy: 0.4727838258164852\n"
     ]
    }
   ],
   "source": [
    "dummy_classifier = DummyClassifier(strategy = 'uniform', random_state = 17)\n",
    "dummy_classifier.fit(feature_train, target_train)\n",
    "random_accuracy = dummy_classifier.score(feature_valid, target_valid)\n",
    "print(f\"Random guessing accuracy: {random_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random guessing accuracy using a uniform strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the logistic regression model on the training set: 0.7520746887966805\n",
      "Accuracy of the logistic regression model on the validation set: 0.7262830482115086\n"
     ]
    }
   ],
   "source": [
    "model_logistic = LogisticRegression(random_state = 17, solver = 'liblinear')\n",
    "model_logistic.fit(feature_train, target_train) \n",
    "score_train_logistic = model_logistic.score(feature_train, target_train)  \n",
    "score_valid_logistic = model_logistic.score(feature_valid, target_valid)  \n",
    "\n",
    "print('Accuracy of the logistic regression model on the training set:', score_train_logistic)\n",
    "print(\"Accuracy of the logistic regression model on the validation set:\",score_valid_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieved a decent score but underperformed compared to the other models, likely because it captures linear relationships rather than complex patterns. Its performance suggests the data signal is driven by feature values, but non-linear models may better capture the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Final Test Score: 0.76049766718507\n"
     ]
    }
   ],
   "source": [
    "final_score_logistic = model_logistic.score(feature_test, target_test)\n",
    "print(f\"Logistic Regression Final Test Score: {final_score_logistic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested the model against the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Best Score: 0.7682737169517885\n",
      "Random Forest Best n_estimators: 24\n"
     ]
    }
   ],
   "source": [
    "best_score_forest = 0\n",
    "best_est_forest = 0\n",
    "for est in range (1, 25):\n",
    "    model_forest = RandomForestClassifier(random_state = 17, n_estimators = est)\n",
    "    model_forest.fit(feature_train, target_train)\n",
    "    score_forest = model_forest.score(feature_valid, target_valid)\n",
    "    if score_forest > best_score_forest:\n",
    "        best_score_forest = score_forest\n",
    "        best_est_forest = est\n",
    "final_forest = RandomForestClassifier(random_state = 17, n_estimators = best_est_forest)\n",
    "final_forest.fit(feature_train, target_train)\n",
    "print(f\"Random Forest Classifier Best Score: {best_score_forest}\")\n",
    "print(f\"Random Forest Best n_estimators: {best_est_forest}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest model achieved good accuracy and performed best with 24 estimators in the tested range (1–24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Final Score: 0.7978227060653188\n"
     ]
    }
   ],
   "source": [
    "final_score_forest = final_forest.score(feature_test, target_test)\n",
    "print(f\"Random Forest Classifier Final Score: {final_score_forest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested the model against the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Best Score: 0.7776049766718507\n",
      "Decision Tree Best max_depth: 7\n"
     ]
    }
   ],
   "source": [
    "best_score_tree = 0\n",
    "best_depth_tree = 0\n",
    "for depth in range(1, 21):\n",
    "    model_tree = DecisionTreeClassifier(random_state=17, max_depth = depth)\n",
    "    model_tree.fit(feature_train, target_train)\n",
    "    score_tree = model_tree.score(feature_valid, target_valid)\n",
    "    if score_tree > best_score_tree:\n",
    "        best_score_tree = score_tree\n",
    "        best_depth_tree = depth\n",
    "final_tree = DecisionTreeClassifier(random_state=17, max_depth=best_depth_tree)\n",
    "final_tree.fit(feature_train, target_train)\n",
    "\n",
    "print(f\"Decision Tree Best Score: {best_score_tree}\")\n",
    "print(f\"Decision Tree Best max_depth: {best_depth_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree model achieved comparable accuracy to the Random Forest and performed best with a max_depth of 7 in the tested range (1–20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Final Test Score: 0.7978227060653188\n"
     ]
    }
   ],
   "source": [
    "final_score_tree = final_tree.score(feature_test, target_test)\n",
    "print(f\"Decision Tree Final Test Score: {final_score_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models were evaluated on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random guessing accuracy: 0.4727838258164852\n",
      "Baseline accuracy: 0.6765163297045101\n",
      "Logistic Regression Final Test Score: 0.76049766718507\n",
      "Random Forest Classifier Final Score: 0.7978227060653188\n",
      "Decision Tree Final Test Score: 0.7978227060653188\n"
     ]
    }
   ],
   "source": [
    "print(f\"Random guessing accuracy: {random_accuracy}\")\n",
    "print(f\"Baseline accuracy: {baseline_accuracy}\")\n",
    "print(f\"Logistic Regression Final Test Score: {final_score_logistic}\")\n",
    "print(f\"Random Forest Classifier Final Score: {final_score_forest}\")\n",
    "print(f\"Decision Tree Final Test Score: {final_score_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation: Deploy the Decision Tree Classifier initially because it matches the Random Forest's accuracy but is faster at inference. Monitor the Random Forest as the dataset grows, since it may become more accurate with additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
